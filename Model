import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import networkx as nx
import scipy.sparse as sp
from scipy.spatial.distance import pdist, squareform
from sklearn.neighbors import NearestNeighbors

class CNNnet(nn.Module):
    def __init__(self, output_dim=32):
        super(CNNnet, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(10, 10), stride=(5, 5), bias=True)
        self.bn1 = nn.BatchNorm2d(16)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5), stride=(3, 3), bias=True)
        self.bn2 = nn.BatchNorm2d(32)
        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)
        self.fc = nn.Linear(32 * 2 * 2, output_dim)

    def forward(self, x):
        x = self.pool(nn.ReLU()(self.bn1(self.conv1(x))))
        x = self.pool(nn.ReLU()(self.bn2(self.conv2(x))))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class GraphConvolution(nn.Module):
    def __init__(self, in_features, out_features, num_nodes):
        super(GraphConvolution, self).__init__()
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        self.bias = nn.Parameter(torch.FloatTensor(out_features))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        nn.init.zeros_(self.bias)

    def forward(self, input, adj):
        support = torch.matmul(input, self.weight)
        output = torch.matmul(adj, support)
        return output + self.bias


class CorticalLayer(nn.Module):
    def __init__(self, in_features, out_features, num_nodes, dropout):
        super(CorticalLayer, self).__init__()
        self.gc1 = GraphConvolution(in_features, out_features, num_nodes)
        self.dropout = dropout
        self.bn = nn.BatchNorm1d(out_features)

    def forward(self, x, adj):
        x = self.gc1(x, adj)
        x = F.relu(x)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.bn(x)
        return x


class MultiLayerCorticalNetwork(nn.Module):
    def __init__(self, graph, stimulus_dim, num_channels, signal_length, num_layers=4, hidden_dim=32, dropout=0.25, device='cpu'):
        super(MultiLayerCorticalNetwork, self).__init__()
        self.num_channels = num_channels
        self.signal_length = signal_length
        self.num_layers = num_layers
        self.device = device
        self.num_nodes = len(graph.nodes)

        # Initialize the adjacency matrix as a learnable parameter
        adj = nx.adjacency_matrix(graph).todense()
        self.adj = nn.Parameter(torch.FloatTensor(adj), requires_grad=True)
        self.to(device)

        # Stimulus encoder
        self.stimulus_encoder = nn.Sequential(
            nn.Linear(stimulus_dim, hidden_dim * self.num_nodes)
        )

        # Cortical layers
        self.cortical_layers = nn.ModuleList()
        for i in range(num_layers):
            self.cortical_layers.append(CorticalLayer(hidden_dim, hidden_dim, self.num_nodes, dropout))

        # Inter-layer connections
        self.inter_layer_connections = nn.ModuleList()
        for i in range(num_layers - 1):
            self.inter_layer_connections.append(
                nn.Sequential(
                    nn.Linear(hidden_dim * self.num_nodes, hidden_dim * self.num_nodes),
                    nn.ReLU(),
                    nn.BatchNorm1d(hidden_dim * self.num_nodes)
                )
            )

        # Output layer
        self.output_layer = nn.Sequential(
            nn.BatchNorm1d(hidden_dim * self.num_nodes),
            nn.Linear(hidden_dim * self.num_nodes, num_channels * signal_length)
        )

    def normalize_adj(self, adj):
        adj = adj + torch.eye(adj.size(0)).to(self.device)  # Ensure it's on the correct device
        rowsum = adj.sum(1) + 1e-10
        d_inv_sqrt = torch.pow(rowsum, -0.5).flatten()
        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.
        d_mat_inv_sqrt = torch.diag(d_inv_sqrt)
        return d_mat_inv_sqrt @ adj @ d_mat_inv_sqrt

    def forward(self, stimulus):
        # Ensure input is on the correct device
        stimulus = stimulus.to(self.device)
        batch_size = stimulus.size(0)

        # Normalize the learnable adjacency matrix
        adj = self.normalize_adj(self.adj)

        # Encode stimulus
        x = self.stimulus_encoder(stimulus)
        x = x.view(batch_size, self.num_nodes, -1)

        # Process through cortical layers with inter-layer connections
        for i, layer in enumerate(self.cortical_layers):
            if i == 0:
                layer_output = layer(x, adj)  # First graph layer
            else:
                # Inter-layer connection (only applied if it's not the first graph layer)
                x_proj = self.inter_layer_connections[i-1](layer_output.view(batch_size, -1)).view(batch_size, self.num_nodes, -1)
                layer_output = layer_output + x_proj
                layer_output = layer(layer_output, adj)  # Subsequent graph layers

        # Use the final layer's output directly
        final_output = layer_output.view(batch_size, -1)

        # Generate output for all channels
        output = self.output_layer(final_output)
        output = output.view(batch_size, self.num_channels, self.signal_length)
        return output




def advanced_loss(output, target):
    return F.mse_loss(output, target)


def preprocess_images(images):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        #transforms.Normalize(mean=[0.485], std=[0.2])
    ])
    return torch.stack([transform(Image.fromarray(img)) for img in images])
